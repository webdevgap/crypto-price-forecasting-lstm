{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":96164,"databundleVersionId":12993472,"isSourceIdPinned":false,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Comprehensive Crypto Market Prediction**","metadata":{}},{"cell_type":"markdown","source":"**Objective**: This notebook implements a complete solution for predicting short-term crypto price movements. The strategy involves in-depth data exploration, advanced feature engineering, a robust time-series validation framework, and a final ensemble model to maximize performance.","metadata":{}},{"cell_type":"markdown","source":"# Setup and Configuration","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom scipy.stats import pearsonr\nimport warnings\nimport gc\n\n# Suppress warnings for a cleaner output\nwarnings.filterwarnings('ignore')\n\nclass Config:\n    \"\"\"Holds all major configuration parameters for the pipeline.\"\"\"\n    TRAIN_PATH = \"/kaggle/input/drw-crypto-market-prediction/train.parquet\"\n    TEST_PATH = \"/kaggle/input/drw-crypto-market-prediction/test.parquet\"\n    SUBMISSION_PATH = \"/kaggle/input/drw-crypto-market-prediction/sample_submission.csv\"\n\n    # The exact feature list from the 0.107 script\n    FEATURES = [\n        \"X863\", \"X856\", \"X344\", \"X598\", \"X862\", \"X385\", \"X852\", \"X603\", \"X860\", \"X674\",\n        \"X415\", \"X345\", \"X137\", \"X855\", \"X174\", \"X302\", \"X178\", \"X532\", \"X168\", \"X612\",\n        \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\", \"volume\", \"X888\", \"X421\", \"X333\"\n    ]\n    \n    LABEL_COLUMN = \"label\"\n    N_FOLDS = 3\n    RANDOM_STATE = 42\n\n    # Hyperparameters from the 0.107 script\n    XGB_PARAMS = {\n        \"tree_method\": \"hist\", \"device\": \"gpu\", \"colsample_bylevel\": 0.4778, \n        \"colsample_bynode\": 0.3628, \"colsample_bytree\": 0.7107, \"gamma\": 1.7095,\n        \"learning_rate\": 0.02213, \"max_depth\": 20, \"max_leaves\": 12, \n        \"min_child_weight\": 16, \"n_estimators\": 1667, \"subsample\": 0.06567,\n        \"reg_alpha\": 39.3524, \"reg_lambda\": 75.4484, \"verbosity\": 0, \n        \"random_state\": RANDOM_STATE\n    }\n    LGBM_PARAMS = {\n        \"boosting_type\": \"gbdt\", \"device\": \"cpu\", \"n_jobs\": -1, \"verbose\": -1,\n        \"random_state\": RANDOM_STATE, \"colsample_bytree\": 0.5039, \"learning_rate\": 0.01260,\n        \"min_child_samples\": 20, \"min_child_weight\": 0.1146, \"n_estimators\": 915,\n        \"num_leaves\": 145, \"reg_alpha\": 19.2447, \"reg_lambda\": 55.5046,\n        \"subsample\": 0.9709, \"max_depth\": 9\n    }\n    \n    LEARNERS = [\n        {\"name\": \"xgb\", \"Estimator\": XGBRegressor, \"params\": XGB_PARAMS},\n        {\"name\": \"lgbm\", \"Estimator\": LGBMRegressor, 'params': LGBM_PARAMS}\n    ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering and Utility Functions\n","metadata":{}},{"cell_type":"code","source":"def create_time_decay_weights(n: int, decay: float = 0.95) -> np.ndarray:\n    \"\"\"Creates exponentially decaying weights, giving more importance to recent data.\"\"\"\n    positions = np.arange(n)\n    normalized = positions / (n - 1)\n    weights = decay ** (1.0 - normalized)\n    return weights * n / weights.sum()\n\ndef feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Creates the same set of features as the high-scoring script.\"\"\"\n    df = df.copy()\n    df['volume_weighted_sell'] = df['sell_qty'] * df['volume']\n    df['buy_sell_ratio'] = df['buy_qty'] / (df['sell_qty'] + 1e-8)\n    df['selling_pressure'] = df['sell_qty'] / (df['volume'] + 1e-8)\n    df['effective_spread_proxy'] = np.abs(df['buy_qty'] - df['sell_qty']) / (df['volume'] + 1e-8)\n    df['log_volume'] = np.log1p(df['volume'])\n    df['bid_ask_imbalance'] = (df['bid_qty'] - df['ask_qty']) / (df['bid_qty'] + df['ask_qty'] + 1e-8)\n    df['order_flow_imbalance'] = (df['buy_qty'] - df['sell_qty']) / (df['buy_qty'] + df['sell_qty'] + 1e-8)\n    df['liquidity_ratio'] = (df['bid_qty'] + df['ask_qty']) / (df['volume'] + 1e-8)\n    return df\n\ndef get_model_slices(n_samples: int):\n    \"\"\"Defines the different time-based windows of the training data.\"\"\"\n    return [\n        {\"name\": \"full_data\", \"cutoff\": 0},\n        {\"name\": \"last_75pct\", \"cutoff\": int(0.25 * n_samples)},\n        {\"name\": \"last_50pct\", \"cutoff\": int(0.50 * n_samples)},\n    ]\n\nprint(\"Utility functions and feature engineering logic defined.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"--- Loading and Preparing Data ---\")\n\ntrain_df = pd.read_parquet(Config.TRAIN_PATH, columns=Config.FEATURES + [Config.LABEL_COLUMN])\ntest_df = pd.read_parquet(Config.TEST_PATH, columns=Config.FEATURES)\n\nprint(f\"Loaded data - Train: {train_df.shape}, Test: {test_df.shape}\")\n\ntrain_df = feature_engineering(train_df).dropna().reset_index(drop=True)\ntest_df = feature_engineering(test_df).fillna(0)\n\n# Update feature list to include newly engineered features\nConfig.FEATURES = [col for col in train_df.columns if col != Config.LABEL_COLUMN]\nX = train_df[Config.FEATURES]\ny = train_df[Config.LABEL_COLUMN]\nX_test = test_df[Config.FEATURES]\n\nprint(f\"Final data - Train: {X.shape}, Test: {X_test.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading and Preparation","metadata":{}},{"cell_type":"code","source":"def load_and_process_data():\n    \"\"\"Load, engineer features, and select best features efficiently\"\"\"\n    print(\"Loading data...\")\n    \n    # Load only necessary columns initially for memory efficiency\n    initial_cols = Config.CORE_FEATURES + [Config.LABEL_COLUMN]\n    train_df = pd.read_parquet(Config.TRAIN_PATH, columns=initial_cols)\n    test_df = pd.read_parquet(Config.TEST_PATH, columns=Config.CORE_FEATURES)\n    submission_df = pd.read_csv(Config.SUBMISSION_PATH)\n    \n    print(f\"Initial data loaded - Train: {train_df.shape}, Test: {test_df.shape}\")\n    \n    # Apply feature engineering\n    print(\"Engineering features...\")\n    train_df = feature_engineering(train_df)\n    test_df = feature_engineering(test_df)\n    \n    # Remove original base features (keep engineered ones)\n    to_remove = [\"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\", \"volume\"]\n    train_df = train_df.drop(columns=to_remove)\n    test_df = test_df.drop(columns=to_remove)\n    \n    print(f\"After feature engineering - Train: {train_df.shape}, Test: {test_df.shape}\")\n    \n    # Smart feature selection on recent data\n    print(\"Performing smart feature selection...\")\n    selected_features = smart_feature_selection(\n        train_df, \n        Config.LABEL_COLUMN, \n        sample_size=Config.FEATURE_SELECTION_SAMPLE_SIZE,\n        top_k=Config.TARGET_FEATURES\n    )\n    \n    # Keep only selected features\n    train_df = train_df[selected_features + [Config.LABEL_COLUMN]]\n    test_df = test_df[selected_features]\n    \n    # Memory optimization\n    print(\"Optimizing memory usage...\")\n    train_df = reduce_mem_usage(train_df, \"train\")\n    test_df = reduce_mem_usage(test_df, \"test\")\n    \n    # Update config with selected features\n    Config.FEATURES = selected_features\n    \n    print(f\"Final data - Train: {train_df.shape}, Test: {test_df.shape}\")\n    print(f\"Selected features: {len(Config.FEATURES)}\")\n    \n    return train_df.reset_index(drop=True), test_df.reset_index(drop=True), submission_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Time-Slice Ensemble Training\n","metadata":{}},{"cell_type":"code","source":"def train_xgb_model(X_train, y_train, X_valid, y_valid, X_test, sample_weights=None):\n    \"\"\"Train optimized XGBoost model\"\"\"\n    model = XGBRegressor(**XGB_PARAMS)\n    \n    # Fit with early stopping for efficiency\n    model.fit(\n        X_train, y_train, \n        sample_weight=sample_weights,\n        eval_set=[(X_valid, y_valid)],\n        early_stopping_rounds=100,\n        verbose=False\n    )\n    \n    valid_pred = model.predict(X_valid)\n    test_pred = model.predict(X_test)\n    \n    return valid_pred, test_pred, model\n\n\n\ndef train_and_evaluate(train_df, test_df):\n    \"\"\"Train models with focus on recent data patterns\"\"\"\n    n_samples = len(train_df)\n    model_slices = get_model_slices(n_samples)\n    \n    # Initialize predictions\n    oof_preds = {s[\"name\"]: np.zeros(n_samples) for s in model_slices}\n    test_preds = {s[\"name\"]: np.zeros(len(test_df)) for s in model_slices}\n    feature_importance = {s[\"name\"]: np.zeros(len(Config.FEATURES)) for s in model_slices}\n    \n    kf = KFold(n_splits=Config.N_FOLDS, shuffle=False)\n    \n    for fold, (train_idx, valid_idx) in enumerate(kf.split(train_df), start=1):\n        print(f\"\\n--- Fold {fold}/{Config.N_FOLDS} ---\")\n        X_valid = train_df.iloc[valid_idx][Config.FEATURES]\n        y_valid = train_df.iloc[valid_idx][Config.LABEL_COLUMN]\n        X_test = test_df[Config.FEATURES]\n        \n        for s in model_slices:\n            cutoff = s[\"cutoff\"]\n            slice_name = s[\"name\"]\n            \n            # Use recent data slice\n            recent_df = train_df.iloc[cutoff:].reset_index(drop=True)\n            rel_idx = train_idx[train_idx >= cutoff] - cutoff\n            \n            if len(rel_idx) == 0:\n                continue\n                \n            X_train = recent_df.iloc[rel_idx][Config.FEATURES]\n            y_train = recent_df.iloc[rel_idx][Config.LABEL_COLUMN]\n            \n            # Create time decay weights for recent emphasis\n            sample_weights = create_time_decay_weights(len(recent_df))[rel_idx]\n            \n            print(f\"  Training {slice_name}: {len(X_train)} samples\")\n            \n            try:\n                valid_pred, test_pred, model = train_xgb_model(\n                    X_train, y_train, X_valid, y_valid, X_test, sample_weights\n                )\n                \n                # Store predictions for validation samples in this slice\n                mask = valid_idx >= cutoff\n                if mask.any():\n                    oof_preds[slice_name][valid_idx[mask]] = valid_pred[mask]\n                \n                # For samples outside the slice, use the most comprehensive slice\n                if cutoff > 0 and (~mask).any():\n                    oof_preds[slice_name][valid_idx[~mask]] = \\\n                        oof_preds[\"recent_95pct\"][valid_idx[~mask]]\n                \n                test_preds[slice_name] += test_pred\n                feature_importance[slice_name] += model.feature_importances_\n                \n                # Compute validation score\n                valid_corr = pearsonr(y_valid, valid_pred)[0]\n                print(f\"    {slice_name} validation correlation: {valid_corr:.4f}\")\n                \n            except Exception as e:\n                print(f\"    Error in {slice_name}: {str(e)}\")\n                continue\n    \n    # Average test predictions across folds\n    for slice_name in test_preds:\n        test_preds[slice_name] /= Config.N_FOLDS\n        feature_importance[slice_name] /= Config.N_FOLDS\n    \n    return oof_preds, test_preds, feature_importance\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Ensemble and Submission","metadata":{}},{"cell_type":"code","source":"# ===== Ensemble and Submission =====\ndef create_smart_ensemble(train_df, oof_preds, test_preds):\n    \"\"\"Create weighted ensemble based on recent performance\"\"\"\n    print(\"\\nEvaluating slice performance...\")\n    \n    slice_scores = {}\n    ensemble_weights = {}\n    \n    for slice_name in oof_preds:\n        # Evaluate on recent data (more relevant for crypto)\n        recent_idx = int(0.8 * len(train_df))  # Last 20% for evaluation\n        recent_true = train_df.iloc[recent_idx:][Config.LABEL_COLUMN]\n        recent_pred = oof_preds[slice_name][recent_idx:]\n        \n        # Remove zeros (unvalidated samples)\n        valid_mask = recent_pred != 0\n        if valid_mask.sum() > 0:\n            score = pearsonr(recent_true[valid_mask], recent_pred[valid_mask])[0]\n            slice_scores[slice_name] = score\n            print(f\"  {slice_name}: {score:.4f} (recent data correlation)\")\n        else:\n            slice_scores[slice_name] = 0\n    \n    # Compute ensemble weights (higher weight for better recent performance)\n    total_score = sum(max(0, score) for score in slice_scores.values())\n    if total_score > 0:\n        ensemble_weights = {k: max(0, v) / total_score for k, v in slice_scores.items()}\n    else:\n        # Equal weights if all scores are poor\n        ensemble_weights = {k: 1.0 / len(slice_scores) for k in slice_scores}\n    \n    print(\"\\nEnsemble weights:\")\n    for slice_name, weight in ensemble_weights.items():\n        print(f\"  {slice_name}: {weight:.3f}\")\n    \n    # Create weighted ensemble\n    ensemble_test = np.zeros(len(test_preds[list(test_preds.keys())[0]]))\n    for slice_name, weight in ensemble_weights.items():\n        ensemble_test += weight * test_preds[slice_name]\n    \n    return ensemble_test, slice_scores, ensemble_weights\n\ndef create_submission(train_df, oof_preds, test_preds, submission_df):\n    \"\"\"Create optimized submission\"\"\"\n    \n    # Create smart ensemble\n    ensemble_pred, slice_scores, weights = create_smart_ensemble(train_df, oof_preds, test_preds)\n    \n    # Evaluate ensemble performance\n    best_slice = max(slice_scores.items(), key=lambda x: x[1])\n    print(f\"\\nBest individual slice: {best_slice[0]} ({best_slice[1]:.4f})\")\n    \n    # Create submission\n    submission = submission_df.copy()\n    submission[\"prediction\"] = ensemble_pred\n    submission.to_csv(\"submission.csv\", index=False)\n    \n    print(f\"\\nSubmission created with ensemble prediction\")\n    print(f\"Ensemble uses {len([w for w in weights.values() if w > 0.01])} slices\")\n    \n    return ensemble_pred","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main function","metadata":{}},{"cell_type":"code","source":"# ===== Main Execution =====\nif __name__ == \"__main__\":\n    print(\"=== Enhanced Crypto Prediction with Smart Feature Selection ===\\n\")\n    \n    # Load and process data\n    print(\"Step 1: Loading and processing data...\")\n    train_df, test_df, submission_df = load_and_process_data()\n    \n    # Train models\n    print(\"\\nStep 2: Training models on recent data slices...\")\n    oof_preds, test_preds, feature_importance = train_and_evaluate(train_df, test_df)\n    \n    # Create submission\n    print(\"\\nStep 3: Creating optimized submission...\")\n    final_pred = create_submission(train_df, oof_preds, test_preds, submission_df)\n    \n    # Print feature importance\n    print(\"\\nTop 15 most important features:\")\n    avg_importance = np.mean(list(feature_importance.values()), axis=0)\n    feature_importance_pairs = list(zip(Config.FEATURES, avg_importance))\n    feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)\n    \n    for i, (feat, imp) in enumerate(feature_importance_pairs[:15]):\n        print(f\"  {i+1:2d}. {feat:35s} - Importance: {imp:.4f}\")\n    \n    print(\"\\n=== Processing Complete! ===\")\n    print(\"Files created:\")\n    print(\"- submission.csv (optimized ensemble)\")\n    print(f\"- Used {len(Config.FEATURES)} carefully selected features\")\n    print(f\"- Focused on recent crypto market patterns\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}